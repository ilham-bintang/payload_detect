{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "appropriate-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "careful-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(\"new_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pharmaceutical-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_filenames = []\n",
    "for f in filenames:\n",
    "    if \".txt\" in f:\n",
    "        clean_filenames.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "modular-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for name in clean_filenames:\n",
    "    with open(\"new_data/\" + name, \"rb\") as f:\n",
    "        text = f.readlines()\n",
    "    texts.append({\n",
    "        \"class\": name,\n",
    "        \"list_text\": text\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "paperback-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "for t in texts:\n",
    "    str_text = []\n",
    "    for tex in t['list_text']:\n",
    "        try:\n",
    "            str_text.append(tex.decode())\n",
    "        except Exception:\n",
    "            str_text.append(tex)\n",
    "        final_data.append({\n",
    "            \"class\": t['class'].split(\".\")[0],\n",
    "            \"list_text\": str_text\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "allied-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hired-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "improved-ballot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>list_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>select_version</td>\n",
       "      <td>[UNION SELECT @@VERSION,SLEEP(5),3\\n, UNION SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>select_version</td>\n",
       "      <td>[UNION SELECT @@VERSION,SLEEP(5),3\\n, UNION SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>select_version</td>\n",
       "      <td>[UNION SELECT @@VERSION,SLEEP(5),3\\n, UNION SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>select_version</td>\n",
       "      <td>[UNION SELECT @@VERSION,SLEEP(5),3\\n, UNION SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>select_version</td>\n",
       "      <td>[UNION SELECT @@VERSION,SLEEP(5),3\\n, UNION SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>select</td>\n",
       "      <td>[UNION ALL SELECT 1\\n, UNION ALL SELECT 1,2\\n,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>select</td>\n",
       "      <td>[UNION ALL SELECT 1\\n, UNION ALL SELECT 1,2\\n,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>select</td>\n",
       "      <td>[UNION ALL SELECT 1\\n, UNION ALL SELECT 1,2\\n,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>select</td>\n",
       "      <td>[UNION ALL SELECT 1\\n, UNION ALL SELECT 1,2\\n,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>select</td>\n",
       "      <td>[UNION ALL SELECT 1\\n, UNION ALL SELECT 1,2\\n,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              class                                          list_text\n",
       "0    select_version  [UNION SELECT @@VERSION,SLEEP(5),3\\n, UNION SE...\n",
       "1    select_version  [UNION SELECT @@VERSION,SLEEP(5),3\\n, UNION SE...\n",
       "2    select_version  [UNION SELECT @@VERSION,SLEEP(5),3\\n, UNION SE...\n",
       "3    select_version  [UNION SELECT @@VERSION,SLEEP(5),3\\n, UNION SE...\n",
       "4    select_version  [UNION SELECT @@VERSION,SLEEP(5),3\\n, UNION SE...\n",
       "..              ...                                                ...\n",
       "166          select  [UNION ALL SELECT 1\\n, UNION ALL SELECT 1,2\\n,...\n",
       "167          select  [UNION ALL SELECT 1\\n, UNION ALL SELECT 1,2\\n,...\n",
       "168          select  [UNION ALL SELECT 1\\n, UNION ALL SELECT 1,2\\n,...\n",
       "169          select  [UNION ALL SELECT 1\\n, UNION ALL SELECT 1,2\\n,...\n",
       "170          select  [UNION ALL SELECT 1\\n, UNION ALL SELECT 1,2\\n,...\n",
       "\n",
       "[171 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "photographic-possible",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['select_version', 'select_version_user', 'select'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "talented-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "optical-level",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Config\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "config.num_labels = len(df['class'].unique())\n",
    "\n",
    "# Instantiate model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "generic-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "amber-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_dict = {    \n",
    "}\n",
    "label_2_index = {}\n",
    "for i, d in enumerate(df['class'].unique()):\n",
    "    index_dict[i] = d\n",
    "    label_2_index[d] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "significant-midnight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'select_version': 0, 'select_version_user': 1, 'select': 2},\n",
       " {0: 'select_version', 1: 'select_version_user', 2: 'select'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_2_index, index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "basic-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "alive-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentSentimentDataset(Dataset):\n",
    "    # Static constant variable\n",
    "    LABEL2INDEX = label_2_index\n",
    "    INDEX2LABEL = index_dict\n",
    "    NUM_LABELS = len(index_dict)\n",
    "    \n",
    "    def __init__(self, dataset, tokenizer, no_special_token=False, *args, **kwargs):\n",
    "        dataset['class'] = dataset['class'].apply(lambda lab: self.LABEL2INDEX[lab])\n",
    "        self.data = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.no_special_token = no_special_token\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data.iloc[index, :]\n",
    "#         print(data)\n",
    "        text, classes = data['list_text'][0], data['class']\n",
    "#         print(text, classes)\n",
    "        subwords = self.tokenizer.encode(text, add_special_tokens=not self.no_special_token)\n",
    "        return np.array(subwords), np.array(classes), data['list_text'][0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)    \n",
    "\n",
    "class DocumentSentimentDataLoader(DataLoader):\n",
    "    def __init__(self, max_seq_len=512, *args, **kwargs):\n",
    "        super(DocumentSentimentDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = self._collate_fn\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def _collate_fn(self, batch):\n",
    "\n",
    "        batch_size = len(batch)\n",
    "        max_seq_len = max(map(lambda x: len(x[0]), batch))\n",
    "        max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        \n",
    "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n",
    "        sentiment_batch = np.zeros((batch_size, 1), dtype=np.int64)\n",
    "        \n",
    "        seq_list = []\n",
    "        for i, (subwords, sentiment, raw_seq) in enumerate(batch):\n",
    "            subwords = subwords[:max_seq_len]\n",
    "            subword_batch[i,:len(subwords)] = subwords\n",
    "            mask_batch[i,:len(subwords)] = 1\n",
    "            sentiment_batch[i,0] = sentiment\n",
    "#             print(sentiment_batch)\n",
    "            \n",
    "            seq_list.append(raw_seq)\n",
    "        return subword_batch, mask_batch, sentiment_batch, seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "based-plenty",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bintang/miniconda3/envs/github/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_dataset = DocumentSentimentDataset(data_train, tokenizer, lowercase=True)\n",
    "test_dataset = DocumentSentimentDataset(data_test, tokenizer, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hidden-russia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bintang/miniconda3/envs/github/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "train_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=True)  \n",
    "test_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "occupational-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_sequence_classification(model, batch_data, i2w, is_test=False, device='cpu', **kwargs):\n",
    "    # Unpack batch data\n",
    "    if len(batch_data) == 3:\n",
    "        (subword_batch, mask_batch, label_batch) = batch_data\n",
    "        token_type_batch = None\n",
    "    elif len(batch_data) == 4:\n",
    "        (subword_batch, mask_batch, token_type_batch, label_batch) = batch_data\n",
    "    \n",
    "    # Prepare input & label\n",
    "    subword_batch = torch.LongTensor(subword_batch)\n",
    "    mask_batch = torch.FloatTensor(mask_batch)\n",
    "    token_type_batch = torch.LongTensor(token_type_batch) if token_type_batch is not None else None\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "            \n",
    "    if device == \"cuda\":\n",
    "        subword_batch = subword_batch.cuda()\n",
    "        mask_batch = mask_batch.cuda()\n",
    "        token_type_batch = token_type_batch.cuda() if token_type_batch is not None else None\n",
    "        label_batch = label_batch.cuda()\n",
    "\n",
    "    # Forward model\n",
    "    outputs = model(subword_batch, attention_mask=mask_batch, token_type_ids=token_type_batch, labels=label_batch)\n",
    "    loss, logits = outputs[:2]\n",
    "    \n",
    "    # generate prediction & label list\n",
    "    list_hyp = []\n",
    "    list_label = []\n",
    "    hyp = torch.topk(logits, 1)[1]\n",
    "    for j in range(len(hyp)):\n",
    "        list_hyp.append(i2w[hyp[j].item()])\n",
    "        list_label.append(i2w[label_batch[j][0].item()])\n",
    "        \n",
    "    return loss, list_hyp, list_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "middle-botswana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'select_version': 0, 'select_version_user': 1, 'select': 2}\n",
      "{0: 'select_version', 1: 'select_version_user', 2: 'select'}\n"
     ]
    }
   ],
   "source": [
    "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "print(w2i)\n",
    "print(i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "higher-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "contained-philadelphia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/bintang/miniconda3/envs/github/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "(Epoch 1) TRAIN LOSS:0.5679 LR:0.00000300: 100%|██████████| 5/5 [00:02<00:00,  2.19it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5679 ACC:0.97 F1:0.96 REC:0.97 PRE:0.96 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bintang/miniconda3/envs/github/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "VALID LOSS:0.5151 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00: 100%|██████████| 2/2 [00:00<00:00,  2.41it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.5151 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bintang/miniconda3/envs/github/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "(Epoch 2) TRAIN LOSS:0.5262 LR:0.00000300: 100%|██████████| 5/5 [00:02<00:00,  2.21it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5262 ACC:0.99 F1:0.99 REC:1.00 PRE:0.99 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bintang/miniconda3/envs/github/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "VALID LOSS:0.4487 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00: 100%|██████████| 2/2 [00:00<00:00,  2.35it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.4487 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bintang/miniconda3/envs/github/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "(Epoch 3) TRAIN LOSS:0.4650 LR:0.00000300: 100%|██████████| 5/5 [00:02<00:00,  2.17it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4650 ACC:0.99 F1:0.99 REC:1.00 PRE:0.99 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bintang/miniconda3/envs/github/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "VALID LOSS:0.4040 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00: 100%|██████████| 2/2 [00:00<00:00,  2.34it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.4040 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bintang/miniconda3/envs/github/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "(Epoch 4) TRAIN LOSS:0.4276 LR:0.00000300: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4276 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bintang/miniconda3/envs/github/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "VALID LOSS:0.3800 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00: 100%|██████████| 2/2 [00:00<00:00,  2.46it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.3800 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bintang/miniconda3/envs/github/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "(Epoch 5) TRAIN LOSS:0.3950 LR:0.00000300: 100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]/home/bintang/miniconda3/envs/github/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.3950 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3150 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00: 100%|██████████| 2/2 [00:00<00:00,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.3150 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    " \n",
    "    total_train_loss = 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "    for i, batch_data in enumerate(train_pbar):\n",
    "        # Forward model\n",
    "\n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss = loss.item()\n",
    "        total_train_loss = total_train_loss + tr_loss\n",
    "\n",
    "        # Calculate metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "\n",
    "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1), get_lr(optimizer)))\n",
    "\n",
    "    # Calculate train metric\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
    "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
    "\n",
    "    # Evaluate on validation\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        batch_seq = batch_data[-1]        \n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "        \n",
    "        # Calculate total loss\n",
    "        valid_loss = loss.item()\n",
    "        total_loss = total_loss + valid_loss\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "\n",
    "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "        \n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
    "        total_loss/(i+1), metrics_to_string(metrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "colored-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "worst-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "def document_sentiment_metrics_fn(list_hyp, list_label):\n",
    "    metrics = {}\n",
    "    metrics[\"ACC\"] = accuracy_score(list_label, list_hyp)\n",
    "    metrics[\"F1\"] = f1_score(list_label, list_hyp, average='macro')\n",
    "    metrics[\"REC\"] = recall_score(list_label, list_hyp, average='macro')\n",
    "    metrics[\"PRE\"] = precision_score(list_label, list_hyp, average='macro')\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "mobile-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_to_string(metric_dict):\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append('{}:{:.2f}'.format(key, value))\n",
    "    return ' '.join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "blocked-exercise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: id or 1=1 UNION ALL SELECT @@VERSION,USER(),SLEEP(5)-- | Label : select_version (72.771%)\n"
     ]
    }
   ],
   "source": [
    "text = 'id or 1=1 UNION ALL SELECT @@VERSION,USER(),SLEEP(5)--'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-brisbane",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
